I"}I<h2 id="problem-preface">Problem preface</h2>

<p>You’re a detective, and you’re currently working to track down a notorious drug kingpin. <strong>If you can find his base-of-operations</strong>, you’ll be able to build a case against him.</p>

<p><img src="http://localhost:4000/assets/inverse-prob/detective.jpg" alt="circle city" /></p>

<p>You receive a tip that this individual is meeting someone at the city center in 20 mins. The center of the city is marked by a monument and surrounded by a large circular park, so you’ll have to walk. <strong>Can you make it in time?</strong>
It is exactly <strong>1 mile to the center</strong>, and you know you can <strong>walk a mile in 19.5 minutes</strong>. Therefore, it will take you <strong>19.5 mins to reach the center</strong>, you can make it.</p>

<h2 id="the-forward-problem">The forward problem</h2>

<p>This is a forward problem, <strong>you know the parameters of the world and you are interested in predicting an outcome</strong>. Here, we know the distance we want to travel and the speed at which we can travel. From this, we’re able to predict the time it will take (outcome):</p>

\[\begin{align}
t &amp;= d \cdot s \\
&amp;= 19.5
\end{align}\]

<p>wherein $d$ is the 1 mile distance and $s$ is your walking speed.</p>

<p>You reach the park center and see the drug kingpin talking with another person! You strategically pick a place to sit nearby and fake-read a newspaper. <strong>You overhear him mention it took him 20 mins to walk to this location</strong>. From the information provided, <strong>do you know the location of the kingpin’s base-of-operation</strong>? It took them the about the same time to reach the same point, is this person is your neighbor?</p>

<h2 id="the-inverse-problem">The inverse problem</h2>

<p>This is the <a href="https://en.wikipedia.org/wiki/Inverse_problem">inverse problem</a>. <strong>We have observed an outcome (time it took to walk), and we want to infer a property of the world (starting location)</strong>.</p>

<h3 id="complications-with-the-inverse-problem">Complications with the inverse problem</h3>

<p>So, can we tell if this person is our neighbor? <strong>No</strong>.</p>

<p>Very often, <strong>inverse problems are ill posed</strong>. We do not have enough information to completely solve the problem: we don’t know how fast they walk and, even if we did, the solution is not unique (discussed below).</p>

<p>If we want to find a solution to this incomplete problem, <strong>we will be required to leverage assumptions and prior knowledge</strong>. For example, let’s say we take an educated guess that, based on their build, this individual walks <em>about</em> the same speed as us. We formulate it mathematically by saying they walk a mile in $(19.5 \pm .5)$ minutes.</p>

<h3 id="solving-the-inverse-problem-with-mcmc">Solving the inverse problem with MCMC</h3>

<p>We define the forward problem as</p>

\[\begin{align}
F(x_1, x_2, s) &amp;= s \cdot \sqrt{x_1^2 + x_2^2} \\
&amp;= t
\end{align}\]

<p>where $(x_1, x_2)$ is a given starting location and $s$ is the walking speed. We can then propose (randomly guess) a set of candidate points and run them through the forward problem, we then see if the solution makes sense by comparing the result to our observation, 20 min. <strong>If the result matches our observation, we’ll save that candidate starting location as a plausible solution.</strong> We assume we are completely ignorant about their starting location, so we pull our random guesses from a uniform distribution across the entire city (2 miles x 2 miles).</p>

<p>We’ve already estimated this person walks about as fast as us. So, for each proposed starting location, we’ll also have to generate a walking speed and we’ll sample this from a Normal distribution.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>Lastly, we don’t know if it took them <em>exactly</em> 20 mins to walk. Typically, people tend to round times to give an number divisible by 5 in conversation (e.g. 12.2 minutes becomes 12 minutes), so we should incorporate this into our model. This is an “observational error”.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>We now have our full Bayesian model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x1 ~ Uniform(-2, 2)         # sample starting location in x1
x2 ~ Uniform(-2, 2)         # sample starting location in x2
s ~ Normal(19.5, .5)        # sample walking speed
t = F(s, x1, x2)            # run forward problem to get predicted time
y ~ Normal(t, 3/2)          # predicted time considering observational error
</code></pre></div></div>

<p>To do anything useful with it we’ll need to leverage MCMC. For python, we have a few options at our disposal; I’ve chosen <a href="http://pyro.ai/numpyro/">numpyro</a> for this example and use the NUTS MCMC sampler.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">numpyro</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="s">'x1'</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">numpyro</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="s">'x2'</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">numpyro</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="s">'s'</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">19.5</span><span class="p">,</span> <span class="p">.</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">numpyro</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="s">'obs'</span><span class="p">,</span> <span class="n">dist</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="p">)</span>


<span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">NUM_WARMUP</span><span class="p">,</span> <span class="n">NUM_SAMPLES</span><span class="p">)</span>
<span class="n">mcmc</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng_key_</span><span class="p">,</span> <span class="n">obs</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">20</span><span class="p">]))</span>
<span class="n">mcmc</span><span class="p">.</span><span class="n">print_summary</span><span class="p">()</span>
</code></pre></div></div>

<p>We run our model and we get the following results:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        mean       std    median      5.0%     95.0%     n_eff     r_hat
 S     19.48      0.50     19.48     18.68     20.32   8971.39      1.00
X1      0.00      0.73      0.00     -1.03      1.04   5148.41      1.00
X2     -0.02      0.73     -0.03     -1.05      1.02   5363.95      1.00
</code></pre></div></div>

<p>This says that the average inferred starting location is $(0,0)$ …basically the location we’re standing. This doesn’t make sense. Let’s look at the samples to understand this more.</p>

<h4 id="interpreting-our-results">Interpreting our results</h4>

<p><img src="http://localhost:4000/assets/inverse-prob/1st_city_map.png" alt="circle city" /></p>

<p>Ah, this makes perfect sense. All possible locations are a ring with radius ~ 1 mile - if we average all of these we get the center of the ring, $(0,0)$.</p>

<h3 id="obtaining-more-information">Obtaining more information</h3>

<p>Obviously, we still can’t tell if this person is our neighbor - they could live anywhere around this ring.</p>

<table style="width:100%">
  <tr>
    <td style="border:1px solid #dddddd; padding:8px; background: #EAEAEA"> <small>I use this example because this really stresses the meat of your job as a data scientist (well, the meat of your job is cleaning data, but this is what gets you paid). Someone who runs a model and blindly trust whatever number is returned is likely to be mislead. Instead, you need a healthy dosage of extreme skepticism to dig though the model and find how it might be misleading you. Once you find what's wrong, you need to think about what piece(s) of information will help narrow the result.</small></td>
  </tr>
</table>

<p>As you’re eavesdropping on this conversation, you notice they are holding a sandwich bag. You know this deli - it’s a popular local spot. The people in the immediate neighborhood go to it all the time, but it’s only a hole-in-the-wall so people further away don’t really know about it. However, if someone from that neighborhood moves across the city, they’ll still drive over to get their fix. This is useful information to us because it provides some information on the positional variables.</p>

<p>We fold this into our model by updating our prior guess on their starting location. We will assume a Laplacian distribution based on what we know about the deli.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p>

<p>Our model is now:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x1 ~ Laplacian(-1.5, .1)    # sample starting location in x1
x2 ~ Laplacian(1, .1)       # sample starting location in x2
s ~ Normal(19.5, .5)        # sample walking speed
t = F(s, x1, x2)            # run forward problem to get predicted time
y ~ Normal(t, 3/2)          # predicted time considering observational error
</code></pre></div></div>

<p>We run the model again we get the following samples:</p>

<p><img src="http://localhost:4000/assets/inverse-prob/2nd_city_map.png" alt="circle city" /></p>

<h3 id="drawing-conclusions">Drawing conclusions</h3>

<p>You can qualitatively tell that more information is needed to improve our model; but, for the sake of this example, we’ll stop here. <strong>What can we do with the results that we have?</strong></p>

<h4 id="what-is-the-most-probable-location">What is the most probable location?</h4>

<p>The most probable location is not the average of these samples (remember how that lead us astray in our first attempt?). Instead, we need to find the point with the highest density of accepted starting locations, the <a href="https://en.wikipedia.org/wiki/Mode_(statistics)#Example_for_a_skewed_distribution">mode</a>. To do this we’ll approximate the samplings as a pdf using a Gaussian kernel density estimation.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> We can then find the maximum point using gradient descent.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gaussian_kde</span>

<span class="n">kde</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]))</span>
<span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">kde</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">x: array([-0.74703862,  0.76423531])</code></p>

<p>We have now identified the highest probable location of the kingpin <em>based on our assumptions</em>.</p>

<p><img src="http://localhost:4000/assets/inverse-prob/kp_city_map.png" alt="circle city" /></p>

<h4 id="what-is-the-probably-they-live-near-you">What is the probably they live near you?</h4>

<p>If we wanted to answer the original question: <strong>is this person is our neighbor?</strong> we can find the probability of this given our MCMC samples. We’ll define our neighborhood as $\pm 0.5$ miles around our house.</p>

<p><img src="http://localhost:4000/assets/inverse-prob/neighbor_city_map.png" alt="circle city" /></p>

<p>We can find this very easily using the samples obtained from our MCMC model. The probability is the number of samples inside our neighborhood, divided by the total number of samples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x1</span> <span class="o">&lt;</span> <span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x1</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x2</span> <span class="o">&lt;</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x2</span> <span class="o">&gt;</span> <span class="o">-</span><span class="p">.</span><span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Probability the kingpin is your neighbor: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">P</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">Probability the kingpin is your neighbor: 22.68%</code></p>

<h4 id="decision-making">Decision making</h4>

<p>Lastly, how can we use this to inform our actions?</p>

<p>How much time should we spend looking for the base of operations in our neighborhood? Well, if we have 5 days to find him, we should only spend 22.68% of it in our neighborhood, or about 1 day and 3 hours.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></p>

<h3 id="final-remarks">Final remarks</h3>

<p>Hopefully this gives you an idea of how to leverage MCMC to solve inverse problems. You can view the full code <a href="https://gist.github.com/bdhammel/8db6eb110e678ed54d12c4fffb62bc8a">on github</a>.</p>

<hr />
<h3 id="references">References</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>If you ask “why a normal distribution” I strongly recommend this youtube video from <a href="https://youtu.be/h5aPo5wXN8E?t=506">Statistical Rethinking Winter 2019 Lecture 03</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Let’s assume that 99.7% of people round to the nearest 5 baised on their watch-hand integer. e.g. 3 is rounded to 5 but 2.9 is rounded to 0. The 99.7 percentile is 2$\sigma$ so our standard dieviation is $3/2$ <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>If you you want to know more about the NUTS MCMC sampler I strongly recommend this blog post <a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">on MCMC sampling methods</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>This does a pretty good job reflecting the highest probability being near the mean, while supporting outliers (people who use to live in the neighborhood and have moved) <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html">Kernel density estimation is a way to estimate the probability density function (PDF) of a random variable in a non-parametric way</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="http://people.duke.edu/~ccc14/sta-663-2016/16A_MCMC.html">MCMC island hopping example</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET